{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-17T13:06:18.716420Z",
     "start_time": "2025-05-17T13:06:03.814085Z"
    }
   },
   "source": [
    "from data.create_dataloaders import create_dataloaders\n",
    "from model.smooth_cross_entropy import smooth_crossentropy\n",
    "from model.wide_res_net import WideResNet\n",
    "import torch\n",
    "\n",
    "from train import OptimizerType, train_multiple_models\n",
    "\n",
    "batch_size = 128\n",
    "threads = 2\n",
    "\n",
    "depth = 16\n",
    "width_factor = 8\n",
    "dropout = 0.0\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = WideResNet(depth, width_factor, dropout, in_channels=3, labels=10).to(device)\n",
    "train_dataloder, val_dataloader, test_dataloader = create_dataloaders(dataset_name=\"cifar10\",\n",
    "                                                                      batch_size=batch_size,\n",
    "                                                                      num_workers=threads)\n",
    "\n",
    "configs = [\n",
    "    {\"model\": model, \"criterion\": smooth_crossentropy, \"optimizer\": {\"optimizer_type\": OptimizerType.SGD}, \"num_epochs\": 2, \"model_name\": \"WRN-SGD\"},\n",
    "    {\"model\": model, \"criterion\": smooth_crossentropy, \"optimizer\": {\"optimizer_type\": OptimizerType.SAM}, \"num_epochs\": 2, \"model_name\": \"WRN-SAM\"},\n",
    "    {\"model\": model, \"criterion\": smooth_crossentropy, \"optimizer\": {\"optimizer_type\": OptimizerType.ADAM}, \"num_epochs\": 2, \"model_name\": \"WRN-ADAM\"},\n",
    "]\n",
    "\n",
    "train_multiple_models(configs, train_dataloder, val_dataloader, test_dataloader, \"cifar10\", device=device)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">WRN-SGD</strong> at: <a href='https://wandb.ai/jhostic-fer/WRN-SGD/runs/8446l169' target=\"_blank\">https://wandb.ai/jhostic-fer/WRN-SGD/runs/8446l169</a><br> View project at: <a href='https://wandb.ai/jhostic-fer/WRN-SGD' target=\"_blank\">https://wandb.ai/jhostic-fer/WRN-SGD</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250517_150452-8446l169/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/Users/jurahostic/Documents/Fakultet/Seminar 2/SAM_evaluation/wandb/run-20250517_150612-78my09j2</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jhostic-fer/WRN-SGD/runs/78my09j2' target=\"_blank\">WRN-SGD</a></strong> to <a href='https://wandb.ai/jhostic-fer/WRN-SGD' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/jhostic-fer/WRN-SGD' target=\"_blank\">https://wandb.ai/jhostic-fer/WRN-SGD</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/jhostic-fer/WRN-SGD/runs/78my09j2' target=\"_blank\">https://wandb.ai/jhostic-fer/WRN-SGD/runs/78my09j2</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model: WRN-SGD\n",
      "\n",
      "==================================================\n",
      "Epoch 1/2\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/352 [00:00<?, ?it/s]/Users/jurahostic/anaconda3/envs/seminar_2/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Training:   0%|          | 0/352 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 27\u001B[39m\n\u001B[32m     17\u001B[39m train_dataloder, val_dataloader, test_dataloader = create_dataloaders(dataset_name=\u001B[33m\"\u001B[39m\u001B[33mcifar10\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     18\u001B[39m                                                                       batch_size=batch_size,\n\u001B[32m     19\u001B[39m                                                                       num_workers=threads)\n\u001B[32m     21\u001B[39m configs = [\n\u001B[32m     22\u001B[39m     {\u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m: model, \u001B[33m\"\u001B[39m\u001B[33mcriterion\u001B[39m\u001B[33m\"\u001B[39m: smooth_crossentropy, \u001B[33m\"\u001B[39m\u001B[33moptimizer\u001B[39m\u001B[33m\"\u001B[39m: {\u001B[33m\"\u001B[39m\u001B[33moptimizer_type\u001B[39m\u001B[33m\"\u001B[39m: OptimizerType.SGD}, \u001B[33m\"\u001B[39m\u001B[33mnum_epochs\u001B[39m\u001B[33m\"\u001B[39m: \u001B[32m2\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mmodel_name\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mWRN-SGD\u001B[39m\u001B[33m\"\u001B[39m},\n\u001B[32m     23\u001B[39m     {\u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m: model, \u001B[33m\"\u001B[39m\u001B[33mcriterion\u001B[39m\u001B[33m\"\u001B[39m: smooth_crossentropy, \u001B[33m\"\u001B[39m\u001B[33moptimizer\u001B[39m\u001B[33m\"\u001B[39m: {\u001B[33m\"\u001B[39m\u001B[33moptimizer_type\u001B[39m\u001B[33m\"\u001B[39m: OptimizerType.SAM}, \u001B[33m\"\u001B[39m\u001B[33mnum_epochs\u001B[39m\u001B[33m\"\u001B[39m: \u001B[32m2\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mmodel_name\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mWRN-SAM\u001B[39m\u001B[33m\"\u001B[39m},\n\u001B[32m     24\u001B[39m     {\u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m: model, \u001B[33m\"\u001B[39m\u001B[33mcriterion\u001B[39m\u001B[33m\"\u001B[39m: smooth_crossentropy, \u001B[33m\"\u001B[39m\u001B[33moptimizer\u001B[39m\u001B[33m\"\u001B[39m: {\u001B[33m\"\u001B[39m\u001B[33moptimizer_type\u001B[39m\u001B[33m\"\u001B[39m: OptimizerType.ADAM}, \u001B[33m\"\u001B[39m\u001B[33mnum_epochs\u001B[39m\u001B[33m\"\u001B[39m: \u001B[32m2\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mmodel_name\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mWRN-ADAM\u001B[39m\u001B[33m\"\u001B[39m},\n\u001B[32m     25\u001B[39m ]\n\u001B[32m---> \u001B[39m\u001B[32m27\u001B[39m train_multiple_models(configs, train_dataloder, val_dataloader, test_dataloader, \u001B[33m\"\u001B[39m\u001B[33mcifar10\u001B[39m\u001B[33m\"\u001B[39m, device=device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Fakultet/Seminar 2/SAM_evaluation/train.py:72\u001B[39m, in \u001B[36mtrain_multiple_models\u001B[39m\u001B[34m(configs, train_loader, val_loader, test_loader, dataset_name, device, use_wandb)\u001B[39m\n\u001B[32m     62\u001B[39m run = wandb.init(project=config[\u001B[33m'\u001B[39m\u001B[33mmodel_name\u001B[39m\u001B[33m'\u001B[39m], name=config[\u001B[33m'\u001B[39m\u001B[33mmodel_name\u001B[39m\u001B[33m'\u001B[39m], config={\n\u001B[32m     63\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33marchitecture\u001B[39m\u001B[33m\"\u001B[39m: config[\u001B[33m'\u001B[39m\u001B[33mmodel_name\u001B[39m\u001B[33m'\u001B[39m],\n\u001B[32m     64\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mdataset\u001B[39m\u001B[33m\"\u001B[39m: dataset_name,\n\u001B[32m   (...)\u001B[39m\u001B[32m     68\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33moptimizer\u001B[39m\u001B[33m\"\u001B[39m: config[\u001B[33m'\u001B[39m\u001B[33moptimizer\u001B[39m\u001B[33m'\u001B[39m].get(\u001B[33m'\u001B[39m\u001B[33moptimizer_type\u001B[39m\u001B[33m'\u001B[39m, OptimizerType.SGD),\n\u001B[32m     69\u001B[39m })\n\u001B[32m     71\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mTraining model: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m72\u001B[39m trained_model, history = train_model(\n\u001B[32m     73\u001B[39m     model=model,\n\u001B[32m     74\u001B[39m     train_loader=train_loader,\n\u001B[32m     75\u001B[39m     val_loader=val_loader,\n\u001B[32m     76\u001B[39m     test_loader=test_loader,\n\u001B[32m     77\u001B[39m     criterion=criterion,\n\u001B[32m     78\u001B[39m     optimizer=optimizer,\n\u001B[32m     79\u001B[39m     num_epochs=num_epochs,\n\u001B[32m     80\u001B[39m     device=device,\n\u001B[32m     81\u001B[39m     save_dir=save_dir,\n\u001B[32m     82\u001B[39m     early_stopping_patience=early_stopping_patience,\n\u001B[32m     83\u001B[39m     model_name=model_name,\n\u001B[32m     84\u001B[39m     use_wandb=use_wandb\n\u001B[32m     85\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Fakultet/Seminar 2/SAM_evaluation/train.py:187\u001B[39m, in \u001B[36mtrain_model\u001B[39m\u001B[34m(model, train_loader, val_loader, test_loader, criterion, optimizer, num_epochs, device, save_dir, early_stopping_patience, model_name, use_wandb)\u001B[39m\n\u001B[32m    185\u001B[39m predictions = model(inputs)\n\u001B[32m    186\u001B[39m loss = criterion(predictions, labels)\n\u001B[32m--> \u001B[39m\u001B[32m187\u001B[39m loss.backward()\n\u001B[32m    188\u001B[39m optimizer.step()\n\u001B[32m    189\u001B[39m optimizer.zero_grad()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/seminar_2/lib/python3.12/site-packages/torch/_tensor.py:648\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    638\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    639\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    640\u001B[39m         Tensor.backward,\n\u001B[32m    641\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    646\u001B[39m         inputs=inputs,\n\u001B[32m    647\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m648\u001B[39m torch.autograd.backward(\n\u001B[32m    649\u001B[39m     \u001B[38;5;28mself\u001B[39m, gradient, retain_graph, create_graph, inputs=inputs\n\u001B[32m    650\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/seminar_2/lib/python3.12/site-packages/torch/autograd/__init__.py:346\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    337\u001B[39m inputs = (\n\u001B[32m    338\u001B[39m     (inputs,)\n\u001B[32m    339\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(inputs, (torch.Tensor, graph.GradientEdge))\n\u001B[32m   (...)\u001B[39m\u001B[32m    342\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m ()\n\u001B[32m    343\u001B[39m )\n\u001B[32m    345\u001B[39m grad_tensors_ = _tensor_or_tensors_to_tuple(grad_tensors, \u001B[38;5;28mlen\u001B[39m(tensors))\n\u001B[32m--> \u001B[39m\u001B[32m346\u001B[39m grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m    347\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m retain_graph \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    348\u001B[39m     retain_graph = create_graph\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/seminar_2/lib/python3.12/site-packages/torch/autograd/__init__.py:199\u001B[39m, in \u001B[36m_make_grads\u001B[39m\u001B[34m(outputs, grads, is_grads_batched)\u001B[39m\n\u001B[32m    197\u001B[39m     out_numel_is_1 = out.numel() == \u001B[32m1\u001B[39m\n\u001B[32m    198\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m out_numel_is_1:\n\u001B[32m--> \u001B[39m\u001B[32m199\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m    200\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mgrad can be implicitly created only for scalar outputs\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    201\u001B[39m     )\n\u001B[32m    202\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m out_dtype.is_floating_point:\n\u001B[32m    203\u001B[39m     msg = (\n\u001B[32m    204\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mgrad can be implicitly created only for real scalar outputs\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    205\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mout_dtype\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    206\u001B[39m     )\n",
      "\u001B[31mRuntimeError\u001B[39m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "94836be7b0fd2ffc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
